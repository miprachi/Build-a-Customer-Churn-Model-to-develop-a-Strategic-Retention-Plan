{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6e76f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 71>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"Final Customer  churn project\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mAutomatically generated by Colaboratory.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03mLet us load the packages needed for visualization and exploratory analysis.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m     72\u001b[0m uploaded\u001b[38;5;241m=\u001b[39mfiles\u001b[38;5;241m.\u001b[39mupload()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Commented out IPython magic to ensure Python compatibility.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Final Customer  churn project\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1yI2Z1_2aiF4Ha8VvvSC1F5t7L1390cdW\n",
    "\n",
    "## **Customer Churn Prediction**\n",
    "\n",
    "A Bank wants to take care of customer retention for their product; savings accounts. The bank wants you to identify customers likely to churn balances below the minimum balance. You have the customers information such as age, gender, demographics along with their transactions with the bank. Your task as a data scientist would be to predict the propensity to churn for each customer.\n",
    "\n",
    "Each row represents a customer, each column contains attributes related to customer demographics and previous transactions with the bank.\n",
    "\n",
    "**Data Dictionary**\n",
    "\n",
    "There are multiple variables in the dataset which can be cleanly divided in 3 categories:\n",
    "# **Demographic information about customers**\n",
    "**customer_id** - Customer id\n",
    "\n",
    "**vintage** - Vintage of the customer with the bank in number of days\n",
    "\n",
    "**age** - Age of customer\n",
    "\n",
    "**gender** - Gender of customer\n",
    "\n",
    "**dependents** - Number of dependents\n",
    "\n",
    "**occupation** - Occupation of the customer\n",
    "\n",
    "**city**- City of customer (anonymised)\n",
    "\n",
    "# **Bank Related Information for customers**\n",
    "**customer_nw_category** - Net worth of customer (3:Low 2:Medium 1:High)\n",
    "\n",
    "**branch_code -** Branch Code for customer account\n",
    "\n",
    "**days_since_last_transaction** - No of Days Since Last Credit in Last 1 year\n",
    "\n",
    "# **Transactional Information**\n",
    "**current_balance** - Balance as of today\n",
    "\n",
    "**previous_month_end_balance** - End of Month Balance of previous month\n",
    "\n",
    "**average_monthly_balance_prevQ**- Average monthly balances (AMB) in Previous Quarter\n",
    "\n",
    "**average_monthly_balance_prevQ2**- Average monthly balances (AMB) in previous to previous quarter\n",
    "\n",
    "**percent_change_credits** - Percent Change in Credits between last 2 quarters\n",
    "\n",
    "**current_month_credit** - Total Credit Amount current month\n",
    "\n",
    "**previous_month_credit** - Total Credit Amount previous month\n",
    "\n",
    "**current_month_debit** - Total Debit Amount current month\n",
    "\n",
    "**previous_month_debit** - Total Debit Amount previous month\n",
    "\n",
    "**current_month_balance** - Average Balance of current month\n",
    "\n",
    "**previous_month_balance** - Average Balance of previous month\n",
    "\n",
    "**churn** - Average balance of customer falls below minimum balance in the next quarter (1/0).\n",
    "\n",
    "Wow, we can already see there are many features in the data dictionary which we included in our hypothesis.\n",
    "\n",
    "## Loading Packages\n",
    "Let us load the packages needed for visualization and exploratory analysis.\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import files\n",
    "uploaded=files.upload()\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "sns.set(style=\"white\")\n",
    "import io\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\"\"\"## Loading Data\"\"\"\n",
    "\n",
    "df= pd.read_csv(io.BytesIO(uploaded[\"churn_prediction.csv\"]))\n",
    "\n",
    "\"\"\"## Exploratory Data Analysis\"\"\"\n",
    "\n",
    "df.shape, df.columns\n",
    "\n",
    "\"\"\"We have data for 28382 customers with 21 columns. So, essentially we have 20 features and 1 target column which is churn. Let us quickly look at the values for each column.\"\"\"\n",
    "\n",
    "df.iloc[1,:]\n",
    "\n",
    "\"\"\"Alright. Here, we have a mix of categorical, numerical and ordinal variables as shown. There are missing values also in some of the features and we would treat them as a part of preprocessing step when we build the model.\n",
    "\n",
    "##Target Exploration\n",
    "We are trying to predict if the client's account balance drops below the minimum balance prescribed for the customer. \n",
    "Clearly, this is a binary classification problem. Let's look at the target variable and find out how many customers are in the churn category.\n",
    "\"\"\"\n",
    "\n",
    "ax = sns.catplot(y=\"churn\", kind=\"count\", data=df, height=2.6, aspect=2.5, orient='h')\n",
    "\n",
    "df['churn'].value_counts(normalize = True)\n",
    "\n",
    "\"\"\"## Numerical features\n",
    "Let us look at the numerical features. From the description provided in the data dictionary and cell 8, we can see that we have the following numerical features. Let us quickly describe them to check the following:\n",
    "\n",
    "**Count**: Can be used to check for missing value count\n",
    "\n",
    "**Mean**: Mean of the variable\n",
    "\n",
    "**Standard Deviation**: Standard deviation of the variable\n",
    "\n",
    "**Minimum**: Minimum value of the variable\n",
    "\n",
    "**Quantile values**: 25, 50 (median) & 75% quantiles of the variable\n",
    "\n",
    "**Maximum**: Maximum value of the variable.\n",
    "\n",
    "Notice that we will not directly used dtypes function to identify numerical columns but rather used business sense to select numerical features as we have seen from a smaple record, branch code and city code actually represent categories and not some meaningful numerical value.\n",
    "\"\"\"\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "\"\"\"As shown dtypes function puts city and branch code features in the numerical category but that is not the intention.\"\"\"\n",
    "\n",
    "numerical_cols = ['customer_id', 'vintage', 'age', 'dependents', 'customer_nw_category', 'days_since_last_transaction', 'current_balance',\n",
    "       'previous_month_end_balance', 'average_monthly_balance_prevQ', 'average_monthly_balance_prevQ2', \n",
    "       'current_month_credit','previous_month_credit', 'current_month_debit', 'previous_month_debit',\n",
    "       'current_month_balance', 'previous_month_balance']\n",
    "df[numerical_cols].describe()\n",
    "\n",
    "\"\"\"## Lets list down a few key observations:\n",
    "\n",
    "*   Customer ID here is just an id variable identifying a unique customer and has values between 1 and 30301\n",
    "*   On an average, a customer from this set has been with the bank for 2400 days or around 6.5 years\n",
    "*   On an average, a customer has less than 1 dependent and has an average age of 48 years\n",
    "*   A general trend on variables which are related to balances have a wide range with huge outliers, it will key to observe these outliers\n",
    "*   Most of the customers lie in category 2 or 3 for net worth and have on an average done the last transaction 70 days ago. Now the high net worth customers (Category) must have high credit, debit and balance values. Let's verify this using data.\n",
    "\n",
    "## Customer Net worth Category & Balance Features\n",
    "We will use a groupby function to check the mean values of balance features.\n",
    "\"\"\"\n",
    "\n",
    "cols = ['current_balance',\n",
    "       'previous_month_end_balance', 'average_monthly_balance_prevQ', 'average_monthly_balance_prevQ2',\n",
    "        'current_month_credit','previous_month_credit', 'current_month_debit', 'previous_month_debit',\n",
    "       'current_month_balance', 'previous_month_balance']\n",
    "df.groupby(['customer_nw_category'])[cols].mean()\n",
    "\n",
    "\"\"\"So there is clear consistency here as mean values of balance features and the credit/debit features have higher values for net worth category 1 and lower value for net worth categories 2 & 3.\n",
    "\n",
    "The bulk of features are comprised of balance and credit debit features. Let us explore them in detail.\n",
    "\n",
    "## Balance & Credit/Debit Features\n",
    "We will start by looking at average balance in the current month. We will use a histogram to check its distribution.\n",
    "\n",
    "### Average Monthly Balance Features\n",
    "\"\"\"\n",
    "\n",
    "sns.distplot(df['current_month_balance'], kde = False)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Due to the huge outliers in both positive and negative directions, it is very difficult to derive insights from this plot. \n",
    "\n",
    "*   In this case, we could convert such columns to log and then check the distributions.\n",
    "*   However, since there are negative values, it cannot be a direct log conversion as log of negative numbers is not defined.\n",
    "\n",
    "*  To tackle this, we add a positive constant within the log as a correction and to account for negative values we add a constant value within log\n",
    "\"\"\"\n",
    "\n",
    "temp = np.log(df['current_month_balance'] + 6000) \n",
    "\n",
    "sns.distplot(temp, kde = False, bins = 100)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Now, we can see more clearly that this is a right skewed feature and we have much more clarity on its distribution. \n",
    "Let us use subplot to quickly look at more numerical features together and see trends.\n",
    "\"\"\"\n",
    "\n",
    "# Numerical Features\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "xmin = 7\n",
    "xmax = 16\n",
    "# Current Month Average Balance\n",
    "temp = np.log(df['current_month_balance'] + 6000) # To account for negative values we add a constant value within log\n",
    "ax1.set_xlim([xmin,xmax])\n",
    "ax1.set(xlabel='log of average balance of current month')\n",
    "sns.distplot(temp, kde = False, bins = 200, ax = ax1)\n",
    "\n",
    "\n",
    "# Previous month average balance\n",
    "temp = np.log(df['previous_month_balance'] + 6000) # To account for negative values we add a constant value within log\n",
    "ax2.set_xlim([xmin,xmax])\n",
    "ax2.set(xlabel='log of average balance of previous month')\n",
    "sns.distplot(temp, kde = False, bins = 200, ax = ax2)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"## Current Balance today vs Average Monthly Balance in current month\n",
    "\n",
    "As expected the average monthly balance for both months are quite similar and have right skewed histograms as shown. Now let us compare the current month average balance vs current balance as of today.\n",
    "\"\"\"\n",
    "\n",
    "# Numerical Features\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "xmin = 7\n",
    "xmax = 16\n",
    "# Current Month Average Balance\n",
    "temp = np.log(df['current_month_balance'] + 6000) # To account for negative values we add a constant value within log\n",
    "ax1.set_xlim([xmin,xmax])\n",
    "#ax1.set(xlabel='log of average balance of current month')\n",
    "sns.distplot(temp, kde = False, bins = 200, ax = ax1)\n",
    "\n",
    "\n",
    "# Current End of month average balance\n",
    "temp = np.log(df['current_balance'] + 6000) # To account for negative values we add a constant value within log\n",
    "ax2.set_xlim([xmin,xmax])\n",
    "#ax2.set(xlabel='log of month end balance of current  month')\n",
    "sns.distplot(temp, kde = False, bins = 200, ax = ax2)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"*   Here, we can see that the distribution for both lie in almost the same interval, however, there are larger number of values for current balance just below 9 which might have been contributed by the churning customers.\n",
    "*   It might be a good idea to create a feature which is the difference of these 2 variables during the model building process.\n",
    "\n",
    "*  Students are encouraged to do more univariate analysis on other balances and check distributions to find similar insights. \n",
    "\n",
    "Next, in order to understand which of these features might be important to predict the churn, we will do a bivariate analysis.\n",
    "\n",
    "##**Bivariate Analysis**\n",
    "Now, we will check the relationship of the numeric variables along with the target. Again conversion to log is important here as we have a lot of outliers and visualization will be difficult for it.\n",
    "\n",
    "##Churn vs Current & Previous month balances\n",
    "\"\"\"\n",
    "\n",
    "balance_cols = ['current_balance','previous_month_end_balance',\n",
    "                'current_month_balance', 'previous_month_balance']\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "for i in balance_cols:\n",
    "    df1[str('log_')+ i] = np.log(df[i] + 6000)\n",
    "\n",
    "log_balance_cols = df1.columns\n",
    "\n",
    "df1['churn'] = df['churn']\n",
    "\n",
    "\"\"\"We will use the brilliant pairplot function from Seaborn which supports displaying relationship between multiple variables. It displays the scatter plot between a pair of feature and also displays the distribution.\n",
    "\n",
    "Here I have included the following:\n",
    "*   Log of current balance & previous month end balance\n",
    "*  Log of average monthly balance of current and previous month\n",
    "\n",
    "*  Churn is represented by the color here **(Orange - Churn, Blue - Not Churn)**\n",
    "\"\"\"\n",
    "\n",
    "#sns.pairplot(df1,vars=log_balance_cols, hue = 'churn',plot_kws={'alpha':0.1})\n",
    "df1_no_churn = df1[df1['churn'] == 0]\n",
    "sns.pairplot(df1_no_churn,vars=log_balance_cols,plot_kws={'alpha':0.1})\n",
    "plt.show()\n",
    "\n",
    "#sns.pairplot(df1,vars=log_balance_cols, hue = 'churn',plot_kws={'alpha':0.1})\n",
    "df1_churn = df1[df1['churn'] == 1]\n",
    "sns.pairplot(df1_churn,vars=log_balance_cols,plot_kws={'alpha':0.1})\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(df1,vars=log_balance_cols,hue ='churn',plot_kws={'alpha':0.1})\n",
    "plt.show()\n",
    "\n",
    "\"\"\"The distribution for these features look similar. We can make the following conclusions from this:\n",
    "\n",
    "*  There is high correlation between the previous and current month balances which is expected\n",
    "*  The lower balances tend to have higher number of churns which is clear from the scatter plots\n",
    "\n",
    "*  Distribution for the balances are all right skewed\n",
    "\n",
    "\n",
    "## Credit and Debits for current and previous months\n",
    "\n",
    "Total credit and debit amounts for the current and previous can be clubbed into the same category. Let us again use the pair plot to check distributions and scatter plots.\n",
    "\"\"\"\n",
    "\n",
    "cr_dr_cols = ['current_month_credit','previous_month_credit', \n",
    "              'current_month_debit', 'previous_month_debit']\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "for i in cr_dr_cols:\n",
    "    df1[str('log_')+ i] = np.log(df[i])\n",
    "\n",
    "log_dr_cr_cols = df1.columns\n",
    "\n",
    "df1['churn'] = df['churn']\n",
    "\n",
    "sns.pairplot(df1,vars=log_dr_cr_cols, hue = 'churn',plot_kws={'alpha':0.5})\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Both credit and debit patterns show significant difference in distributions for churned and non churned customers.\n",
    "\n",
    "1.  Bimodal distribution/Double Bell Curve shows that there are 2 different types of customers with 2 brackets of credit and debit. Now, during the modeling phase, these could be considered as a seperate set of customers\n",
    "2.  For debit values, we see that there is a significant difference in the distribution for churn and non churn and it might be turn out to be an important feature\n",
    "\n",
    "### Average monthly balance of previous and previous to previous quarters\n",
    "\n",
    "Now, these 2 variables present deeper historical transactions and would help in understanding the trend during the last 2 quarters.\n",
    "\"\"\"\n",
    "\n",
    "q_cols = ['average_monthly_balance_prevQ', 'average_monthly_balance_prevQ2']\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "for i in q_cols:\n",
    "    df1[str('log_')+ i] = np.log(df[i] + 17000)\n",
    "\n",
    "log_q_cols = df1.columns\n",
    "df1['churn'] = df['churn']\n",
    "\n",
    "sns.pairplot(df1,vars=log_q_cols, hue = 'churn',plot_kws={'alpha':0.5})\n",
    "plt.show()\n",
    "\n",
    "\"\"\"The distributions do not have much difference when it comes to churn.\n",
    "\n",
    "However, there are some high negative values in the previous to previous quarters due to which there appears to be a lateral shift. However, if you look at the x-axis, it is still at the same scale for both features.\n",
    "\n",
    "Removing the extreme outliers from the data using the 1 and 99th percentile would help us look at the correct distributions\n",
    "\"\"\"\n",
    "\n",
    "# Remove 1st and 99th percentile and plot\n",
    "\n",
    "df2 = df[['average_monthly_balance_prevQ', 'average_monthly_balance_prevQ2']]\n",
    "\n",
    "low = .01\n",
    "high = .99\n",
    "quant_df = df2.quantile([low, high])\n",
    "print(quant_df)\n",
    "\n",
    "df3 = df2.apply(lambda x: x[(x>quant_df.loc[low,x.name]) & \n",
    "                                    (x < quant_df.loc[high,x.name])], axis=0)\n",
    "\n",
    "q_cols = ['average_monthly_balance_prevQ', 'average_monthly_balance_prevQ2']\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "for i in q_cols:\n",
    "    df1[str('log_')+ i] = np.log(df3[i] + 17000)\n",
    "\n",
    "log_q_cols = df1.columns\n",
    "df1['churn'] = df['churn']\n",
    "\n",
    "sns.pairplot(df1,vars=log_q_cols, hue = 'churn',plot_kws={'alpha':0.5})\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Now, we can clearly see that the distributions are very similar for both the variables and and non churning customers have higher average monthly balances in previous 2 quarters.\n",
    "\n",
    "##**Demographics and Bank Related Information for customers**\n",
    "Revisiting the description for numerical demographic & Bank related customer variables we have:\n",
    "\n",
    "**Numerical**:\n",
    "\n",
    "**vintage** - Vintage of the customer with the bank in number of days\n",
    "\n",
    "**age** - Age of customer\n",
    "\n",
    "**days_since_last_transaction** - No of Days Since Last Credit in Last 1 year\n",
    "\n",
    "**Ordinal**:\n",
    "\n",
    "**dependents** - Number of dependents\n",
    "\n",
    "**customer_nw_category** - Net worth of customer (3:Low 2:Medium 1:High)\n",
    "\n",
    "KDE plot can be used for numerical variables on the same axis to quickly compare the distributions for churning and non churning customers. It basically plots the approximate churn rate against each normal variable. This is exactly similar to what we did in the pairplot with distributions but here we would look at them separately since they represent entirely different variables.\n",
    "\n",
    "## Days Since Last Transaction\n",
    "\"\"\"\n",
    "\n",
    "# KDE Plot Smoothens out even if there are no values for a value\n",
    "def kdeplot(feature):\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.title(\"KDE Plot for {}\".format(feature))\n",
    "    ax0 = sns.kdeplot(df[df['churn'] == 0][feature].dropna(), color= 'dodgerblue', label= 'Churn - 0')\n",
    "    ax1 = sns.kdeplot(df[df['churn'] == 1][feature].dropna(), color= 'orange', label= 'Churn - 1')\n",
    "\n",
    "kdeplot('days_since_last_transaction')\n",
    "\n",
    "\"\"\"There is no significant difference between the distributions for churning and non churning customers when it comes to days since last transaction.\n",
    "\n",
    "##Age & Vintage\n",
    "\"\"\"\n",
    "\n",
    "kdeplot('age')\n",
    "\n",
    "\"\"\"Similarly, age also does not significantly affect the churning rate. However, customers above 80 years of age less likely to churn.\"\"\"\n",
    "\n",
    "kdeplot('vintage')\n",
    "\n",
    "\"\"\"For most frequent vintage values, the churning customers are slightly higher, while for higher values of vintage, we have mostly non churning customers which is in sync with the age variable.\n",
    "\n",
    "##Categorical features\n",
    "This dataset has 4 categorical features (gender, occupation, city and branch code) as can be inferred from the data dictionary. Now let us have a look at the the number of unique values for each of them.\n",
    "\"\"\"\n",
    "\n",
    "cat_cols = ['gender', 'occupation', 'city','branch_code']\n",
    "\n",
    "for i in range(0,len(cat_cols)):\n",
    "    print(str(cat_cols[i]) + \" - Number of Unique Values: \" + str(df[cat_cols[i]].nunique()))\n",
    "\n",
    "\"\"\"So, there are a large number of unique values for branch code and city. Gender has 2 unique values while occupation has 7. \n",
    "\n",
    "##Univariate Analysis\n",
    "Let us look at each categorical feature and check distribution.\n",
    "\"\"\"\n",
    "\n",
    "color = sns.color_palette()\n",
    "\n",
    "int_level = df['gender'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(int_level.index, int_level.values, alpha=0.8, color=color[1])\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Gender', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Amongst the customers, we have more males than females here. Lets check occupation now.\"\"\"\n",
    "\n",
    "color = sns.color_palette()\n",
    "\n",
    "int_level = df['occupation'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(int_level.index, int_level.values, alpha=0.8, color=color[1])\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Occupation', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Most of the customers are self employed, followed by salaried account holders, retired custumors and very low number of companies.\n",
    "\n",
    "Now, branch code and city code have a lot of unique values and direct visualization will be difficult. Lets see how:\n",
    "\n",
    "##City Code & Branch Code\n",
    "\"\"\"\n",
    "\n",
    "color = sns.color_palette()\n",
    "\n",
    "int_level = df['city'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(int_level.index, int_level.values, alpha=0.8, color=color[1])\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('City', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Now, let us have a look at the frequencies of the top city codes:\"\"\"\n",
    "\n",
    "df['city'].value_counts().head(20)\n",
    "\n",
    "# Convert city variable wrt degree of number of customers\n",
    "df['city_bin'] = df['city'].copy()\n",
    "counts = df.city.value_counts()\n",
    "df.city_bin[df['city'].isin(counts[counts > 900].index)] = 3\n",
    "df.city_bin[df['city'].isin(counts[counts < 900].index) & df['city_bin'].isin(counts[counts >= 350].index)] = 2\n",
    "df.city_bin[df['city'].isin(counts[counts < 350].index) & df['city_bin'].isin(counts[counts >= 100].index)] = 1\n",
    "df.city_bin[df['city'].isin(counts[counts < 100].index)] = 0\n",
    "\n",
    "df['city_bin'] = pd.to_numeric(df['city_bin'], errors='coerce')\n",
    "\n",
    "int_level = df['city_bin'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(int_level.index, int_level.values, alpha=0.8, color=color[1])\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('city bins', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"There are 2 major categories here. Cities with more than 900 occurances and with less than 100 occurances. Similarly we can create bins for branch id and have a look.\"\"\"\n",
    "\n",
    "df['branch_code'].value_counts()\n",
    "\n",
    "# Convert city variable wrt degree of number of customers\n",
    "df['branch_bin'] = df['branch_code'].copy()\n",
    "counts = df.branch_code.value_counts()\n",
    "df.branch_bin[df['branch_code'].isin(counts[counts >= 100].index)] = 2\n",
    "df.branch_bin[df['branch_code'].isin(counts[counts < 100].index) & df['branch_bin'].isin(counts[counts >= 50].index)] = 1\n",
    "df.branch_bin[df['branch_code'].isin(counts[counts < 50].index)] = 0\n",
    "\n",
    "df['branch_bin'] = pd.to_numeric(df['branch_bin'], errors='coerce')\n",
    "\n",
    "df['branch_bin'].value_counts()\n",
    "\n",
    "int_level = df['branch_bin'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(int_level.index, int_level.values, alpha=0.8, color=color[1])\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('branch bins', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"So creating brackets/bins on the basis of frequency is a good idea to quickly analyse a variable with high number of categories.\n",
    "\n",
    "\n",
    "Next, Let us look at some bivariate analysis for categorical variables.\n",
    "\n",
    "\n",
    "## Bivariate Analysis\n",
    "\n",
    "Lets define a function to quickly compare churn rates for different categories in each feature.\n",
    "\"\"\"\n",
    "\n",
    "def barplot_percentages(feature):\n",
    "    #fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "    ax1 = df.groupby(feature)['churn'].value_counts(normalize=True).unstack()\n",
    "    ax1.plot(kind='bar', stacked='True')\n",
    "    int_level = df[feature].value_counts()\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.barplot(int_level.index, int_level.values, alpha=0.8, color=color[1])\n",
    "    plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "    plt.xlabel(str(feature), fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"##**Gender**\"\"\"\n",
    "\n",
    "barplot_percentages(\"gender\")\n",
    "\n",
    "\"\"\"Does not look like a very significant variable as the ratio of churned customers and others is very similar.\n",
    "\n",
    "##**Occupation**\n",
    "\"\"\"\n",
    "\n",
    "barplot_percentages(\"occupation\")\n",
    "\n",
    "\"\"\"Self Employed and salaried have higher churn rate and are the major categories.\n",
    "\n",
    "##**Branch Bins**\n",
    "\"\"\"\n",
    "\n",
    "barplot_percentages(\"branch_bin\")\n",
    "\n",
    "\"\"\"## **City Bins**\"\"\"\n",
    "\n",
    "barplot_percentages(\"city_bin\")\n",
    "\n",
    "\"\"\"Here, we see significant difference for different occupations and certainly would be interesting to use as a feature for prediction of churn. However, city and branch codes have little difference amongst the different types of branches.\n",
    "\n",
    "##Dependents\n",
    "\"\"\"\n",
    "\n",
    "df['dependents'][df['dependents'] > 3] = 3\n",
    "\n",
    "barplot_percentages(\"dependents\")\n",
    "\n",
    "\"\"\"Most customers have no dependents and hence this variable in itself has low variance so it is of little significance.\n",
    "\n",
    "##Customer Net worth Category\n",
    "\"\"\"\n",
    "\n",
    "barplot_percentages(\"customer_nw_category\")\n",
    "\n",
    "\"\"\"Not much difference in customer net worth category when it comes to churn.\n",
    "\n",
    "\n",
    "##Correlation Heatmap\n",
    "Lastly, we will look at the correlation heatmap to check what all variables are correlated and to what extent.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.drop(['customer_id'],\n",
    "        axis=1, inplace=True)\n",
    "corr = df.apply(lambda x: pd.factorize(x)[0]).corr()\n",
    "ax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, \n",
    "                 linewidths=.2, cmap=\"YlGnBu\")\n",
    "\n",
    "\"\"\"*  The balance features are highly correlated as can be seen from the plot\n",
    "*  Other variables have correlations but on the lower side\n",
    "\n",
    "\n",
    "*   Debit values have the highest correlation amongst the balance features\n",
    "*  Interestingly vintage has a considerable correlation with all the balance features which actually makes sense since older customers will tend to have higher balance\n",
    "\n",
    "##**Conclusions**\n",
    "\n",
    "**Average customer Profile**\n",
    "\n",
    "Overall a customer at this bank:\n",
    "\n",
    "\n",
    "*  has no dependents\n",
    "*  has been a customer for last 6 years\n",
    "\n",
    "\n",
    "*   predominantly male\n",
    "*   either self employed or salaried customer\n",
    "\n",
    "**EDA Conclusion for Churn**\n",
    "\n",
    "*  From the sample, around 17% customers are churning\n",
    "*  Current balance and average monthly balance values have a left skewed distribution as observed from the histogram\n",
    "* No significant difference in distributions for average monthly balance and month end balances\n",
    " * Bimodal distribution/Double Bell Curve shows that there are 2 different types of customers with 2 brackets of credit and debit. Now, during the modeling phase, these could be considered as a seperate set of customers\n",
    "* For debit values, we see that there is a significant difference in the distribution for churn and non churn and it might be turn out to be an important feature\n",
    "* For most frequent vintage values, the churning customers are slightly higher, while for higher values of vintage, we have mostly non churning customers which is in sync with the age variable \n",
    "* Gender does not look like a very significant variable as the ratio of churned customers and others is very similar\n",
    "* Self Employed and salaried have higher churn rate and are the most frequently occuring categories.\n",
    "* Not much difference in customer net worth category when it comes to churn\n",
    "\n",
    "#Missing Values\n",
    "\n",
    "Before we go on to build the model, we must look for missing values within the dataset as treating the missing values is a necessary step before we fit a logistic regression model on the dataset.\n",
    "\"\"\"\n",
    "\n",
    "pd.isnull(df).sum()\n",
    "\n",
    "\"\"\"The result of this function shows that there are quite a few missing values in columns gender, dependents, city, days since last transaction and Percentage change in credits. Let us go through each of them 1 by 1 to find the appropriate missing value imputation strategy for each of them.\n",
    "\n",
    "#Gender\n",
    "\n",
    "For a quick recall let us look at the categories within gender column\n",
    "\"\"\"\n",
    "\n",
    "df['gender'].value_counts()\n",
    "\n",
    "\"\"\"So there is a good mix of males and females and arguably missing values cannot be filled with any one of them. We could create a seperate category by assigning the value -1 for all missing values in this column.\n",
    "\n",
    "Before that, first we will convert the gender into 0/1 and then replace missing values with -1.\n",
    "\"\"\"\n",
    "\n",
    "#Convert Gender\n",
    "dict_gender = {'Male': 1, 'Female':0}\n",
    "df.replace({'gender': dict_gender}, inplace = True)\n",
    "\n",
    "df['gender'] = df['gender'].fillna(-1)\n",
    "\n",
    "\"\"\"#Dependents, occupation and city with mode\n",
    "\n",
    "Next, we will have a quick look at the dependents & occupations column and impute with mode as this is sort of an ordinal variable.\n",
    "\"\"\"\n",
    "\n",
    "df['dependents'].value_counts()\n",
    "\n",
    "df['occupation'].value_counts()\n",
    "\n",
    "df['dependents'] = df['dependents'].fillna(0)\n",
    "df['occupation'] = df['occupation'].fillna('self_employed')\n",
    "\n",
    "df['city'] = df['city'].fillna(1020)\n",
    "\n",
    "\"\"\"#Days since Last Transaction\n",
    "\n",
    "A fair assumption can be made on this column as this is number of days since last transaction in 1 year, we can substitute missing values with a value greater than 1 year say 999.\n",
    "\"\"\"\n",
    "\n",
    "df['days_since_last_transaction'] = df['days_since_last_transaction'].fillna(999)\n",
    "\n",
    "\"\"\"#Preprocessing\n",
    "\n",
    "Now, before applying linear model such as logistic regression, we need to scale the data and keep all features as numeric strictly. \n",
    "\n",
    "\n",
    "#Dummies with Multiple Categories\n",
    "\"\"\"\n",
    "\n",
    "# Convert occupation to one hot encoded features\n",
    "df = pd.concat([df,pd.get_dummies(df['occupation'],prefix = str('occupation'),prefix_sep='_')],axis = 1)\n",
    "\n",
    "\"\"\"##**Scaling Numerical Features**\n",
    "Now, we remember that there are a lot of outliers in the dataset especially when it comes to previous and current balance features. Also, the distributions are skewed for these features if you recall from the EDA. We will take 2 steps to deal with that here:\n",
    "\n",
    "1) Log Transformation\n",
    "\n",
    "2) Standard Scaler\n",
    "\n",
    "Standard scaling is anyways a necessity when it comes to linear models and we have done that here after doing log transformation on all balance features.\n",
    "\"\"\"\n",
    "\n",
    "num_cols = ['customer_nw_category', 'current_balance',\n",
    "            'previous_month_end_balance', 'average_monthly_balance_prevQ2', 'average_monthly_balance_prevQ',\n",
    "            'current_month_credit','previous_month_credit', 'current_month_debit', \n",
    "            'previous_month_debit','current_month_balance', 'previous_month_balance']\n",
    "for i in num_cols:\n",
    "    df[i] = np.log(df[i] + 17000)\n",
    "\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(df[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns=num_cols)\n",
    "\n",
    "df_df_og = df.copy()\n",
    "df = df.drop(columns = num_cols,axis = 1)\n",
    "df = df.merge(scaled,left_index=True,right_index=True,how = \"left\")\n",
    "\n",
    "y_all = df.churn\n",
    "df = df.drop(['churn','occupation'],axis = 1)\n",
    "\n",
    "\"\"\"##**Model Building and Evaluation Metrics**\n",
    "Since this is a binary classification problem, we could use the following 2 popular metrics:\n",
    "\n",
    "1.   Recall\n",
    "2.   Area under the Receiver operating characteristic curve\n",
    "\n",
    "Now, we are looking at the recall value here because a customer falsely marked as churn would not be as bad as a customer who was not detected as a churning customer and appropriate measures were not taken by the bank to stop him/her from churning.\n",
    "\n",
    "The ROC AUC is the area under the curve when plotting the (normalized) true positive rate (x-axis) and the false positive rate (y-axis).\n",
    "\n",
    "Our main metric here would be Recall values, while AUC ROC Score would take care of how well predicted probabilites are able to differentiate between the 2 classes.\n",
    "\n",
    "##**Conclusions from EDA**\n",
    "\n",
    "*   For debit values, we see that there is a significant difference in the distribution for churn and non churn and it might be turn out to be an important feature.\n",
    "*   For all the balance features the lower values have much higher proportion of churning customers.\n",
    "*   For most frequent vintage values, the churning customers are slightly higher, while for higher values of vintage, we have mostly non churning customers which is in sync with the age variable \n",
    "*   We see significant difference for different occupations and certainly would be interesting to use as a feature for prediction of churn.\n",
    "\n",
    "Now, we will first split our dataset into test and train and using the above conclusions select columns and build a baseline logistic regression model to check the ROC-AUC Score & the confusion matrix\n",
    "\n",
    "##Baseline Columns\n",
    "\"\"\"\n",
    "\n",
    "baseline_cols = ['current_month_debit', 'previous_month_debit','current_balance','previous_month_end_balance','vintage'\n",
    "                 ,'occupation_retired', 'occupation_salaried','occupation_self_employed', 'occupation_student']\n",
    "df_baseline = df[baseline_cols]\n",
    "\n",
    "\"\"\"### Train Test Split to create a validation set\"\"\"\n",
    "\n",
    "# Splitting the data into Train and Validation set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df_baseline,y_all,test_size=1/3, random_state=11, stratify = y_all)\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, xtrain, ytrain, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "\n",
    "# Compare Algorithms\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.show()\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(xtrain, ytrain)\n",
    "\n",
    "rfc_pred = rfc.predict(xtest)\n",
    "\n",
    "\"\"\"##Classification Report and Confusion Matrix\"\"\"\n",
    "\n",
    "print(confusion_matrix(ytest,rfc_pred))\n",
    "\n",
    "print(classification_report(ytest,rfc_pred))\n",
    "\n",
    "print( accuracy_score(ytest,rfc_pred))\n",
    "\n",
    "\"\"\"Hence, We got our desired result by using Random Forest Machine Learning algorithm.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bd8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
